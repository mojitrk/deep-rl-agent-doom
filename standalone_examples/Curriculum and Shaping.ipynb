{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "insured-allen",
   "metadata": {},
   "source": [
    "<div id=top></div>\n",
    "\n",
    "# Reinforcement Learning with Doom - Reward shaping and curriculum learning\n",
    "\n",
    "Leandro Kieliger\n",
    "contact@lkieliger.ch\n",
    "\n",
    "---\n",
    "## Description\n",
    "\n",
    "In this notebook we are going to significantly improve the learning efficiency of the setup created in the previous part of this series. First, we will see how to modify rewards to incentivize behaviors helping reach the learning objective, a method called \"reward shaping\". In the second part, we will design an adaptive learning process that changes the difficulty of the training environment based on the performance of the agent. \n",
    "\n",
    "\n",
    "### [Part 1 - Reward Shaping](#part_1)\n",
    "* [Action multipliers](#shaping_table)\n",
    "* [Shaped environment wrapper](#shaped_env)\n",
    "\n",
    "    \n",
    "### [Part 2 - Curriculum Learning](#part_2)\n",
    "* [ACS script](#acs_script)\n",
    "* [Curriculum environment wrapper](#curriculum_env)\n",
    "* [Final model](#final_model)\n",
    "    \n",
    "    \n",
    "### [Bonus - Human vs AI, playing against a trained agent](#bonus)\n",
    "\n",
    "### [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-parameter",
   "metadata": {},
   "source": [
    "<div id=part_1></div>\n",
    "\n",
    "# [^](#top) Part 1 - Reward Shaping\n",
    "\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparative-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import typing as t\n",
    "import vizdoom\n",
    "\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common import evaluation, policies\n",
    "from torch import nn\n",
    "\n",
    "from common import envs, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-belle",
   "metadata": {},
   "source": [
    "In the previous notebook we saw that the learning process was very slow. Indeed, even after training more than 2 million steps, our agent barely reached 2 frags per match on average. In comparison, the best bot manages to get around 13 frags. For reference, here is the average performance of six consecutive runs. The shaded area shows the mean error $ \\frac{\\sigma}{\\sqrt{n}}$ for $n=6$.\n",
    "\n",
    "![Comparison performance](./figures/comparison_shaping_1.png)\n",
    "\n",
    "We also discussed one of the main reasons why the model had so much difficulty getting started: sparse rewards. That is, the agent has to execute many steps \"just right\" before it can observe some meaningful reward signal. Indeed, it must manage to move and aim at enemies while repeatedly shooting them in order to (possibly) get some rewards. Such sequences of actions rarely happen by chance. If rewards are rare, it will take a long time to reinforce good behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-wisconsin",
   "metadata": {},
   "source": [
    "<div id=shaping_table></div>\n",
    "\n",
    "## Action multipliers\n",
    "\n",
    "To solve the issue of sparse rewards, we can give our agent small positive rewards for every action we believe is beneficial to the learning process. Here is the list of actions we would like to incentivize as well as the associated rewards:\n",
    "\n",
    "| Action                     | Reward       |\n",
    "| -------------------------- |--------------| \n",
    "| Frag                       |  1 per frag   | \n",
    "| Damaging enemies           |  0.01 per damage point | \n",
    "| Picking up ammunition      |  0.02 per unit |\n",
    "| Using ammunition           | -0.01 per unit | \n",
    "| Picking up health          |  0.02 per health point |\n",
    "| Losing health              | -0.01 per health point |\n",
    "| Picking up armor           |  0.01 per armor point |\n",
    "| Moved distance > 3 units   |  5e-4 per step |\n",
    "| Moved distance < 3 units   | -2.5e-3 per step |\n",
    "\n",
    "Note that players typically have 100 health points and that damage points correspond to the number of enemy health points that were removed. Also, players can typically move at around 16 units per tick. The distance reward is here to avoid \"camping\" behavior. Values have been inspired and adapted from a paper using this technique to improve their performance: \n",
    "> Wu, Yuxin and Yuandong Tian. “Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning.” ICLR (2017). [PDF](https://research.fb.com/wp-content/uploads/2017/04/paper_camera_ready_small-1.pdf)\n",
    "\n",
    "To modify the rewards we just need to keep track of a few variables and adapt the value before passing it on to the agent. First, we set the action rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "younger-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "# 1 per kill\n",
    "reward_factor_frag = 1.0\n",
    "reward_factor_damage = 0.01\n",
    "\n",
    "# Player can move at ~16.66 units per tick\n",
    "reward_factor_distance = 5e-4\n",
    "penalty_factor_distance = -2.5e-3\n",
    "reward_threshold_distance = 3.0\n",
    "\n",
    "# Pistol clips have 10 bullets\n",
    "reward_factor_ammo_increment = 0.02\n",
    "reward_factor_ammo_decrement = -0.01\n",
    "\n",
    "# Player starts at 100 health\n",
    "reward_factor_health_increment = 0.02\n",
    "reward_factor_health_decrement = -0.01\n",
    "reward_factor_armor_increment = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-entrance",
   "metadata": {},
   "source": [
    "<div id=shaped_env></div>\n",
    "\n",
    "## Shaped environment wrapper\n",
    "Then, we define a game environment wrapper class, just like the one with bots we did in the previous part. It might seem lenghty at first but most of the code is actually computing reward components based on the multipliers defined above. Each component is aggregated in the `shape_rewards` function when performing an action `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eleven-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "from gym import spaces\n",
    "from vizdoom.vizdoom import GameVariable\n",
    "\n",
    "# List of game variables storing ammunition information. Used for keeping track of ammunition-related rewards.\n",
    "AMMO_VARIABLES = [GameVariable.AMMO0, GameVariable.AMMO1, GameVariable.AMMO2, GameVariable.AMMO3, GameVariable.AMMO4,\n",
    "                  GameVariable.AMMO5, GameVariable.AMMO6, GameVariable.AMMO7, GameVariable.AMMO8, GameVariable.AMMO9]\n",
    "\n",
    "# List of game variables storing weapon information. Used for keeping track of ammunition-related rewards.\n",
    "WEAPON_VARIABLES = [GameVariable.WEAPON0, GameVariable.WEAPON1, GameVariable.WEAPON2, GameVariable.WEAPON3,\n",
    "                    GameVariable.WEAPON4,\n",
    "                    GameVariable.WEAPON5, GameVariable.WEAPON6, GameVariable.WEAPON7, GameVariable.WEAPON8,\n",
    "                    GameVariable.WEAPON9]\n",
    "\n",
    "class DoomWithBotsShaped(envs.DoomWithBots):\n",
    "    \"\"\"An environment wrapper for a Doom deathmatch game with bots. \n",
    "    \n",
    "    Rewards are shaped according to the multipliers defined in the notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots, shaping):\n",
    "        super().__init__(game, frame_processor, frame_skip, n_bots)\n",
    "\n",
    "        # Give a random two-letter name to the agent for identifying instances in parallel learning.\n",
    "        self.name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=2))\n",
    "        self.shaping = shaping\n",
    "\n",
    "        # Internal states\n",
    "        self.last_health = 100\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.ammo_state = self._get_ammo_state()\n",
    "        self.weapon_state = self._get_weapon_state()\n",
    "        self.total_rew = self.last_damage_dealt = self.deaths = self.last_frags = self.last_armor = 0\n",
    "\n",
    "        # Store individual reward contributions for logging purposes\n",
    "        self.rewards_stats = {\n",
    "            'frag': 0,\n",
    "            'damage': 0,\n",
    "            'ammo': 0,\n",
    "            'health': 0,\n",
    "            'armor': 0,\n",
    "            'distance': 0,\n",
    "        }\n",
    "        \n",
    "    def step(self, action, array=False):\n",
    "        # Perform the action as usual\n",
    "        state, reward, done, info = super().step(action)\n",
    "        \n",
    "        self._log_reward_stat('frag', reward)\n",
    "\n",
    "        # Adjust the reward based on the shaping table\n",
    "        if self.shaping:\n",
    "            shaped_reward = reward + self.shape_rewards()\n",
    "        else:\n",
    "            shaped_reward = reward\n",
    "\n",
    "        self.total_rew += shaped_reward\n",
    "\n",
    "        return state, shaped_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._print_state()\n",
    "        \n",
    "        state = super().reset()\n",
    "\n",
    "        self.last_health = 100\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.last_armor = self.last_frags = self.total_rew = self.deaths = 0\n",
    "\n",
    "        # Damage count  is not cleared when starting a new episode: https://github.com/mwydmuch/ViZDoom/issues/399\n",
    "        # self.last_damage_dealt = 0\n",
    "\n",
    "        # Reset reward stats\n",
    "        for k in self.rewards_stats.keys():\n",
    "            self.rewards_stats[k] = 0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def shape_rewards(self):\n",
    "        reward_contributions = [\n",
    "            self._compute_damage_reward(),\n",
    "            self._compute_ammo_reward(),\n",
    "            self._compute_health_reward(),\n",
    "            self._compute_armor_reward(),\n",
    "            self._compute_distance_reward(*self._get_player_pos()),\n",
    "        ]\n",
    "\n",
    "        return sum(reward_contributions)\n",
    "    \n",
    "    def _respawn_if_dead(self):\n",
    "        if not self.game.is_episode_finished():\n",
    "            # Check if player is dead\n",
    "            if self.game.is_player_dead():\n",
    "                self.deaths += 1\n",
    "                self._reset_player()\n",
    "\n",
    "    def _compute_distance_reward(self, x, y):\n",
    "        \"\"\"Computes a reward/penalty based on the distance travelled since last update.\"\"\"\n",
    "        dx = self.last_x - x\n",
    "        dy = self.last_y - y\n",
    "\n",
    "        distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "\n",
    "        if distance - reward_threshold_distance > 0:\n",
    "            reward = reward_factor_distance\n",
    "        else:\n",
    "            reward = -reward_factor_distance\n",
    "\n",
    "        self.last_x = x\n",
    "        self.last_y = y\n",
    "        self._log_reward_stat('distance', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_damage_reward(self):\n",
    "        \"\"\"Computes a reward based on total damage inflicted to enemies since last update.\"\"\"\n",
    "        damage_dealt = self.game.get_game_variable(GameVariable.DAMAGECOUNT)\n",
    "        reward = reward_factor_damage * (damage_dealt - self.last_damage_dealt)\n",
    "\n",
    "        self.last_damage_dealt = damage_dealt\n",
    "        self._log_reward_stat('damage', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_health_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total health change since last update.\"\"\"\n",
    "        # When the player is dead, the health game variable can be -999900\n",
    "        health = max(self.game.get_game_variable(GameVariable.HEALTH), 0)\n",
    "\n",
    "        health_reward = reward_factor_health_increment * max(0, health - self.last_health)\n",
    "        health_penalty = reward_factor_health_decrement * min(0, health - self.last_health)\n",
    "        reward = health_reward - health_penalty\n",
    "\n",
    "        self.last_health = health\n",
    "        self._log_reward_stat('health', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_armor_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total armor change since last update.\"\"\"\n",
    "        armor = self.game.get_game_variable(GameVariable.ARMOR)\n",
    "        reward = reward_factor_armor_increment * max(0, armor - self.last_armor)\n",
    "        \n",
    "        self.last_armor = armor\n",
    "        self._log_reward_stat('armor', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_ammo_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total ammunition change since last update.\"\"\"\n",
    "        self.weapon_state = self._get_weapon_state()\n",
    "\n",
    "        new_ammo_state = self._get_ammo_state()\n",
    "        ammo_diffs = (new_ammo_state - self.ammo_state) * self.weapon_state\n",
    "        ammo_reward = reward_factor_ammo_increment * max(0, np.sum(ammo_diffs))\n",
    "        ammo_penalty = reward_factor_ammo_decrement * min(0, np.sum(ammo_diffs))\n",
    "        reward = ammo_reward - ammo_penalty\n",
    "        \n",
    "        self.ammo_state = new_ammo_state\n",
    "        self._log_reward_stat('ammo', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _get_player_pos(self):\n",
    "        \"\"\"Returns the player X- and Y- coordinates.\"\"\"\n",
    "        return self.game.get_game_variable(GameVariable.POSITION_X), self.game.get_game_variable(\n",
    "            GameVariable.POSITION_Y)\n",
    "\n",
    "    def _get_ammo_state(self):\n",
    "        \"\"\"Returns the total available ammunition per weapon slot.\"\"\"\n",
    "        ammo_state = np.zeros(10)\n",
    "\n",
    "        for i in range(10):\n",
    "            ammo_state[i] = self.game.get_game_variable(AMMO_VARIABLES[i])\n",
    "\n",
    "        return ammo_state\n",
    "\n",
    "    def _get_weapon_state(self):\n",
    "        \"\"\"Returns which weapon slots can be used. Available weapons are encoded as ones.\"\"\"\n",
    "        weapon_state = np.zeros(10)\n",
    "\n",
    "        for i in range(10):\n",
    "            weapon_state[i] = self.game.get_game_variable(WEAPON_VARIABLES[i])\n",
    "\n",
    "        return weapon_state\n",
    "\n",
    "    def _log_reward_stat(self, kind: str, reward: float):\n",
    "        self.rewards_stats[kind] += reward\n",
    "\n",
    "    def _reset_player(self):\n",
    "        self.last_health = 100\n",
    "        self.last_armor = 0\n",
    "        self.game.respawn_player()\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.ammo_state = self._get_ammo_state()\n",
    "\n",
    "    def _print_state(self):\n",
    "        super()._print_state()\n",
    "        print('\\nREWARD BREAKDOWN')\n",
    "        print('Agent {} frags: {}, deaths: {}, total reward: {:.2f}'.format(\n",
    "            self.name,\n",
    "            self.last_frags,\n",
    "            self.deaths,\n",
    "            self.total_rew\n",
    "        ))\n",
    "        for k, v in self.rewards_stats.items():\n",
    "            print(f'- {k}: {v:+.1f}')\n",
    "        print('***************************************\\n\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-lambda",
   "metadata": {},
   "source": [
    "We define some helper functions whose task is simply to create a VizDoom game instance and store it with our newly defined environment wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "figured-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv\n",
    "\n",
    "def game_instance(scenario):\n",
    "    \"\"\"Creates a Doom game instance.\"\"\"\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config(f'scenarios/{scenario}.cfg')\n",
    "    game.add_game_args(envs.DOOM_ENV_WITH_BOTS_ARGS)\n",
    "    game.init()\n",
    "    \n",
    "    return game\n",
    "\n",
    "def env_with_bots_shaped(scenario, **kwargs) -> envs.DoomEnv:\n",
    "    \"\"\"Wraps a Doom game instance in an environment with shaped rewards.\"\"\"\n",
    "    game = game_instance(scenario)\n",
    "    return DoomWithBotsShaped(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots_shaped(n_envs=1, **kwargs) -> VecTransposeImage:\n",
    "    \"\"\"Wraps a Doom game instance in a vectorized environment with shaped rewards.\"\"\"\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots_shaped(**kwargs)] * n_envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-wallace",
   "metadata": {},
   "source": [
    "We can now train on the map introduced in the second notebook. The code loading the model, registering the callbacks and starting the learning process has been moved to the `common` module for readability. This way, we can start the training with a single call to `solve_env` which will handle the aspects we have already covered previously.\n",
    "\n",
    "In the part below we define:\n",
    "\n",
    "* Environment parameters such as how much frame skipping, how many bots etc (see notebook 1 & 2).\n",
    "* Agent parameters such as the learning rate, steps per rollout and our custom CNN (see notebook 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "threaded-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.models import CustomCNN\n",
    "\n",
    "scenario = 'deathmatch_simple'\n",
    "\n",
    "# Agent parameters.\n",
    "agent_args = {\n",
    "    'n_epochs': 3,\n",
    "    'n_steps': 4096,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 32,\n",
    "    'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}\n",
    "\n",
    "# Environment parameters.\n",
    "env_args = {\n",
    "    'scenario': scenario,\n",
    "    'frame_skip': 4,\n",
    "    'frame_processor': envs.default_frame_processor,\n",
    "    'n_bots': 8,\n",
    "    'shaping': True\n",
    "}\n",
    "\n",
    "# In the evaluation environment we measure frags only.\n",
    "eval_env_args = dict(env_args)\n",
    "eval_env_args['shaping'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments with bots and shaping.\n",
    "env = vec_env_with_bots_shaped(2, **env_args)\n",
    "eval_env = vec_env_with_bots_shaped(1, **eval_env_args)\n",
    "\n",
    "envs.solve_env(env, eval_env, scenario, agent_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-string",
   "metadata": {},
   "source": [
    "Follow the learning process via Tensorboard. If you run it from the same directory as this notebook, the command is: `tensorboard --logdir=logs/tensorboard`.\n",
    "\n",
    "You should see some noticeable improvement over our previous setup with average rewards starting to rise much earlier in the learning process. In the figure below I have illustrated the average reward curve over 6 consecutive trials. The difference is stunning! Within the same amount of time we were able to obtain a 3x improvement! In addition, we see that our agent is already stronger than programmed bots!\n",
    "\n",
    "But there is more, reward shaping is not the only way of improving the learning performance. In the next part of this notebook we will see how curriculum learning can further boost our setup.\n",
    "\n",
    "![Comparison performance](./figures/comparison_shaping_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-miller",
   "metadata": {},
   "source": [
    "<div id=part_2></div>\n",
    "\n",
    "# [^](#top) Part 2 - Curriculum Learning\n",
    "\n",
    "The concept behind curriculum learning is to make the learning task easy at first and then gradually increase the difficulty as the agent progresses. To implement the idea in a deathmatch environment, we will alter the speed and health of bots based on the performance of the agent. The following table summarises the bots parameters based on the average reward obtained by the agent. The average reward is computed over the last 10 episodes.\n",
    "\n",
    "| Average reward over last 10 episodes | Bot multiplier |\n",
    "| :----------------------------------: |:--------------:| \n",
    "| <=5                                   |  0.1           |\n",
    "| <=10                                  |  0.2           |\n",
    "| <=15                                  |  0.4           |\n",
    "| <=20                                  |  0.6           |\n",
    "| <=25                                  |  0.8           |\n",
    "|  >25                                  |  1.0           |\n",
    "\n",
    "A multiplier of 0.1 means that bots will have 10% of their usual health and will move at only 10% the normal speed. Once the average reward rises above 5, bots will have 20% of their health and speed etc.\n",
    "\n",
    "We can't directly influence the behaviour of bots with Python code. Instead, we need to use ACS scripts. Those scripts are stored in the `.wad` file alongside each map. To read and edit scripts, you can use [Slade](https://slade.mancubus.net/). Using ACS, it is quite easy to modify game variables. The following snippet is all we need for the task. For more information about ACS and what you can do with it, refer to [ZDoom's ACS documentation](https://zdoom.org/wiki/ACS).\n",
    "\n",
    "\n",
    "<div id=acs_script></div>\n",
    "\n",
    "### ACS Script:\n",
    "---\n",
    "```C\n",
    "#include \"zcommon.acs\"\n",
    "\n",
    "global int 0:reward;\n",
    "\n",
    "int difficulty_level = 5;\n",
    "int speed_levels[6] = {0.1, 0.2, 0.4, 0.6, 0.8, 1.0};\n",
    "int health_levels[6] = {10, 20, 40, 60, 80, 100};\n",
    "\n",
    "script 1 OPEN\n",
    "{\n",
    "  Log(s:\"Level loaded\");\n",
    "}\n",
    "\n",
    "script 2 ENTER\n",
    "{\n",
    "  set_actor_skill(ActivatorTID());\n",
    "}\n",
    "\n",
    "script 3 RESPAWN\n",
    "{\n",
    "  set_actor_skill(ActivatorTID());\n",
    "}\n",
    "\n",
    "script \"change_difficulty\" (int new_difficulty_level)\n",
    "{\n",
    "  Log(s:\"Changing difficulty level to: \", d: new_difficulty_level);\n",
    "  \n",
    "  difficulty_level = new_difficulty_level;\n",
    "}\n",
    "\n",
    "function void set_actor_skill(int actor_id)\n",
    "{\n",
    "  if (ClassifyActor(actor_id) & ACTOR_BOT ) {\n",
    "    Log(s:\"Changing difficulty level for bot!\", d: actor_id, d: difficulty_level);\n",
    "    SetActorProperty(actor_id, APROP_Speed , speed_levels[difficulty_level]);\n",
    "    SetActorProperty(actor_id, APROP_Health , health_levels[difficulty_level]);\n",
    "  }\n",
    "}\n",
    "```\n",
    "---\n",
    "<div id=curriculum_env></div>\n",
    "\n",
    "## Curriculum environment wrapper\n",
    "To interact with a function defined in an ACS script we can use the `puke` and `pukename` commands. (The latter allows calling functions by their name. This is a specificity of ACS which originally identified functions using integers.)\n",
    "\n",
    "```Python\n",
    "game.send_game_command(f'pukename <function name> <arguments>')\n",
    "```\n",
    "\n",
    "For more details, have a look at the ZDoom Wiki. Just like for reward shaping, we will subclass the environment wrapper to add the behaviour we need. Also, we need to make sure that the environment used for evaluating the agent's performance is using the normal difficulty (no curriculum applied). Otherwise we would have biased estimates of our agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unnecessary-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "REWARD_THRESHOLDS = [5, 10, 15, 20, 25, 25]\n",
    "\n",
    "class DoomWithBotsCurriculum(DoomWithBotsShaped):\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots, shaping, initial_level=0, max_level=5, rolling_mean_length=10):\n",
    "        super().__init__(game, frame_processor, frame_skip, n_bots, shaping)\n",
    "        \n",
    "        # Initialize ACS script difficulty level\n",
    "        game.send_game_command('pukename change_difficulty 0')\n",
    "        \n",
    "        # Internal state\n",
    "        self.level = initial_level\n",
    "        self.max_level = max_level\n",
    "        self.rolling_mean_length = rolling_mean_length\n",
    "        self.last_rewards = deque(maxlen=rolling_mean_length)\n",
    "\n",
    "    def step(self, action, array=False):\n",
    "        # Perform action step as usual\n",
    "        state, reward, done, infos = super().step(action, array)\n",
    "\n",
    "        # After an episode, check whether difficulty should be increased.\n",
    "        if done:\n",
    "            self.last_rewards.append(self.total_rew)\n",
    "            run_mean = np.mean(self.last_rewards)\n",
    "            print('Avg. last 10 runs of {}: {:.2f}. Current difficulty level: {}'.format(self.name, run_mean, self.level))\n",
    "            if run_mean > REWARD_THRESHOLDS[self.level] and len(self.last_rewards) >= self.rolling_mean_length:\n",
    "                self._change_difficulty()\n",
    "\n",
    "        return state, reward, done, infos\n",
    "\n",
    "    def reset(self):\n",
    "        state = super().reset()\n",
    "        self.game.send_game_command(f'pukename change_difficulty {self.level}')\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _change_difficulty(self):\n",
    "        \"\"\"Adjusts the difficulty by setting the difficulty level in the ACS script.\"\"\"\n",
    "        if self.level < self.max_level:\n",
    "            self.level += 1\n",
    "            print(f'Changing difficulty for {self.name} to {self.level}')\n",
    "            self.game.send_game_command(f'pukename change_difficulty {self.level}')\n",
    "            self.last_rewards = deque(maxlen=self.rolling_mean_length)\n",
    "        else:\n",
    "            print(f'{self.name} already at max level!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-criterion",
   "metadata": {},
   "source": [
    "This is much shorter than the previous wrapper as we only need to check the rolling average reward and optionally send a command to the Doom game instance. Finally, we launch a training session using the wrapper for curriculum learning and wait for 3M steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-neutral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def env_with_bots_curriculum(scenario, **kwargs) -> envs.DoomEnv:\n",
    "    \"\"\"Wraps a Doom game instance in an environment with shaped rewards and curriculum.\"\"\"\n",
    "    game = game_instance(scenario)\n",
    "    return DoomWithBotsCurriculum(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots_curriculum(n_envs=1, **kwargs) -> VecTransposeImage:\n",
    "    \"\"\"Wraps a Doom game instance in a vectorized environment with shaped rewards and curriculum.\"\"\"\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots_curriculum(**kwargs)] * n_envs))\n",
    "\n",
    "# Create environments with bots.\n",
    "env = vec_env_with_bots_curriculum(2, **env_args)\n",
    "eval_env = vec_env_with_bots_shaped(1, **eval_env_args) # Don't use adaptive curriculum for the evaluation env!\n",
    "\n",
    "envs.solve_env(env, eval_env, scenario, agent_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-boards",
   "metadata": {},
   "source": [
    "The learning speed should now be even better than before! By combining reward shaping and curriculum learning we managed to get a 4x increase in performance for the same number of training steps! We are now fully equipped to efficiently train an agent that will outmatch even the best programmed bot beyond a shadow of a doubt. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-yugoslavia",
   "metadata": {},
   "source": [
    "![Comparison performance](./figures/comparison_shaping_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-gather",
   "metadata": {},
   "source": [
    "<div id=final_model></div>\n",
    "\n",
    "## Final model\n",
    "\n",
    "To celebrate the training of the final model, I have created a fancier map that requires more efforts for players to navigate and find enemies. The screenshot below shows an overhead view of the new map. You can find the corresponding `.wad` file on the GitHub repository.\n",
    "\n",
    "![New map](./figures/map_2_scaled.png)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The final model is based on our custom CNN with a couple of changes:\n",
    "\n",
    "* 512 neurons for the first flat fully connected layer.\n",
    "* 256 neurons in a fully connected layer for the **value net**.\n",
    "* 256 neurons in a fully connected layer for the **action net**.\n",
    "\n",
    "The exact code can be found [here](https://github.com/lkiel/rl-doom/blob/develop/src/models/cnn.py). It is probably more complicated than it needs to be due to the possibility to add or not different forms of normalization. Also, the exact number of neurons used is not that important. From my experiments, I have had very good results with less than half of the neurons at each step.\n",
    "\n",
    "Finally, I also used [frame stacking](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecframestack) as it come for free when you use stable-baselines vectorized wrappers but it does not improve significantly the performance of the agent. I have had similar results with frame stacking set to 1.\n",
    "\n",
    "### Training\n",
    "To obtain the final model, I trained with reward shaping and curriculum learning as follows:\n",
    "\n",
    "1. 10M training steps from scratch using a frame skip parameter of 4.\n",
    "1. 10M training steps using the previous result and setting the frame skip parameter to 2.\n",
    "1. 10M training steps using the previous result and setting the frame skip parameter to 1.\n",
    "\n",
    "At the beginning of the training process, the frame skip is set relatively high to speed up the learning. Then, it is progressively reduced to improve the aiming accuracy of the agent. The figure below shows the final \"learning curve\" for this setup. Notice the sharp jump in performance as soon as we allow the agent to skip less frames. Also note that it is more difficult for the agent to get frags on this map due to its more complex structure. Thus, we can't directly compare the performance after 3M steps to the plots shown above.\n",
    "\n",
    "![Best model training](./figures/final_training_rewards.png)\n",
    "\n",
    "Thus, the best model reaches on average 27 frags per game of 2:30 minutes. This is around one frag every 5.5 seconds! Here is an animation of resulting agent in action, destroying the competition:\n",
    "\n",
    "![Final agent](./figures/deathmatch_stack=4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-clerk",
   "metadata": {},
   "source": [
    "<div id=\"bonus\"></div>\n",
    "\n",
    "# Bonus: play against your agent!\n",
    "\n",
    "Want to see how your skills compare to the AI? There are two helper scripts in the `bin` folder:\n",
    "\n",
    "* `demo_deathmatch.sh`\n",
    "* `demo_multiplayer.sh`\n",
    "\n",
    "The first one starts a game of deathmatch with 8 bots and a pretrained agent. Use it if you want a demonstration of what the agent can do. The second one will spawn two instances of Doom, one for a human player and one for the pretrained agent. Each player joins the same deathmatch game with 7 programmed bots. Good luck! \n",
    "\n",
    "Note: Due to the limitation of Jupyter notebooks to run multiple processes in parallel, I could not include the demo directly in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-cigarette",
   "metadata": {},
   "source": [
    "<div id=\"conclusion\"></div>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Over the course of this three-part series we managed to train a reinforcement learning agent to play Doom deathmatch games. We started with a basic setup able to solve very simple scenarios where only a limited number of actions were allowed and where the complexity of the learning task was significantly constrained. We worked our way towards more elaborated tasks by increasing the number of parameters in our model and monitoring closely the learning process to ensure a smooth progression. Finally, we saw how reward shaping and curriculum learning could boost the training to reach good result much quicker.\n",
    "\n",
    "\n",
    "## Potential improvements\n",
    "If you have played a game of deathmatch against a fully trained agent you will have noticed that it is actually quite hard to keep up in terms of score. However, when playing in a 1 versus 1 it remains quite easy to defeat it due to its overall lack of strategy. I've identified three aspects that can be improved:\n",
    "\n",
    "### Memory\n",
    "You might have noticed that the agent has no concept of memory. This means that enemies that are not visible on the screen are immediately forgotten by the agent. Also, the agent does not keep track of places that it has already visited. This is not a big issue when playing against 8 programmed bots as there is always an enemy close by. However, when playing against a single opponent this means that the agent will revisit several times the same location of simply ignore some area of the map it should have explored.\n",
    "\n",
    "A potential improvement here would be to use a model that has a concept of memory like a LSTM neural network. This paper shows that such a model could be used to play Doom effectively.\n",
    "> Lample, Guillaume, and Devendra Singh Chaplot. “Playing FPS Games with Deep Reinforcement Learning.” ArXiv:1609.05521 [Cs], Jan. 2018. arXiv.org, http://arxiv.org/abs/1609.05521.\n",
    "\n",
    "### Difficulty\n",
    "If you have played yourself, you might have noticed that the programmed bots are not the smartest of opponents. They will often get stuck against walls or randomly run across the map. This also means that the amount of strategy needed by our agent to get good rewards is not enormous. It might be interesting to see wether we can increase the performance of the agent by letting it play against versions of itself. Stronger opponents means the agent will potentially learn more interesting strategies.\n",
    "\n",
    "### Aggressivity\n",
    "The agent prefers attacking than picking strategic items or protecting himself. It is hard to point to a single cause but it might be possible to mitigate this behaviour by picking different weights for the reward shaping process or defining new actions to be reinforced altogether."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
