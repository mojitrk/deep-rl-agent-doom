{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: vizdoom[gym] in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (2.5.2)\n",
      "Requirement already satisfied: gym>=0.26.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gym>=0.26.0->vizdoom[gym]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gym>=0.26.0->vizdoom[gym]) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom[gym]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom[gym]) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: stable-baselines3 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (4.10.0.82)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-image in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (2024.5.22)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (24.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 861.2 kB/s eta 0:00:00\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install vizdoom\n",
    "%pip install vizdoom[gym]\n",
    "%pip install gymnasium\n",
    "%pip install stable-baselines3 \n",
    "%pip install opencv-python\n",
    "%pip install scikit-image\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import vizdoom\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config('scenarios/basic.cfg')\n",
    "    game.init()\n",
    "\n",
    "    sample_actions = [\n",
    "        [1, 0, 0],  \n",
    "        [0, 1, 0],  \n",
    "        [0, 0, 1],  \n",
    "    ]\n",
    "\n",
    "    n_episodes = 10\n",
    "    current_episode = 0\n",
    "    \n",
    "    while current_episode < n_episodes:\n",
    "        game.make_action(random.choice(sample_actions))\n",
    "\n",
    "        if game.is_episode_finished():\n",
    "            current_episode += 1\n",
    "            game.new_episode()\n",
    "\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vizdoom as vzd\n",
    "\n",
    "game = vzd.DoomGame()\n",
    "game.load_config(os.path.join(vzd.scenarios_path, \"deathmatch.cfg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from vizdoom import gymnasium_wrapper\n",
    "\n",
    "env = gymnasium.make(\"VizdoomBasic-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choice\n",
    "from time import sleep\n",
    "import vizdoom as vzd\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    game = vzd.DoomGame()\n",
    "\n",
    "    game.set_doom_scenario_path(os.path.join(vzd.scenarios_path, \"basic.wad\"))\n",
    "    game.set_doom_map(\"map01\")\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "    game.set_depth_buffer_enabled(True)\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "    game.set_automap_buffer_enabled(True)\n",
    "    game.set_objects_info_enabled(True)\n",
    "    game.set_sectors_info_enabled(True)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False) \n",
    "    game.set_render_crosshair(False)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False) \n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False)  \n",
    "    game.set_render_messages(False) \n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(\n",
    "        True\n",
    "    ) \n",
    "\n",
    "    game.set_available_buttons(\n",
    "        [vzd.Button.MOVE_LEFT, vzd.Button.MOVE_RIGHT, vzd.Button.ATTACK]\n",
    "    )\n",
    "  \n",
    "    print(\"Available buttons:\", [b.name for b in game.get_available_buttons()])\n",
    "\n",
    "    game.set_available_game_variables([vzd.GameVariable.AMMO2])\n",
    "    print(\n",
    "        \"Available game variables:\",\n",
    "        [v.name for v in game.get_available_game_variables()],\n",
    "    )\n",
    "\n",
    "    game.set_episode_timeout(200)\n",
    "    game.set_episode_start_time(10)\n",
    "    game.set_window_visible(True)\n",
    "\n",
    "\n",
    "    game.set_living_reward(-1)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.init()\n",
    "    \n",
    "    actions = [[True, False, False], [False, True, False], [False, False, True]]\n",
    "\n",
    "    episodes = 10\n",
    "\n",
    "    sleep_time = 1.0 / vzd.DEFAULT_TICRATE \n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(f\"Episode #{i + 1}\")\n",
    "\n",
    "        game.new_episode()\n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            n = state.number\n",
    "            vars = state.game_variables\n",
    "\n",
    "            screen_buf = state.screen_buffer\n",
    "            depth_buf = state.depth_buffer\n",
    "            labels_buf = state.labels_buffer\n",
    "            automap_buf = state.automap_buffer\n",
    "            audio_buf = state.audio_buffer\n",
    "\n",
    "            labels = state.labels\n",
    "\n",
    "            objects = state.objects\n",
    "\n",
    "            sectors = state.sectors\n",
    "\n",
    "            r = game.make_action(choice(actions))\n",
    "\n",
    "            print(f\"State #{n}\")\n",
    "            print(\"Game variables:\", vars)\n",
    "            print(\"Reward:\", r)\n",
    "            print(\"=====================\")\n",
    "\n",
    "            if sleep_time > 0:\n",
    "                sleep(sleep_time)\n",
    "\n",
    "        print(\"Episode finished.\")\n",
    "        print(\"Total reward:\", game.get_total_reward())\n",
    "        print(\"************************\")\n",
    "\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n",
      "Initializing new model\n",
      "\n",
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -50.9 +/- 182.8, min: -390.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -18.9 +/- 169.7, min: -350.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 8.94 minutes\n",
      "\n",
      "Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 46.2 +/- 103.2, min: -385.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 37.7 +/- 116.4, min: -375.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 16.71 minutes\n",
      "\n",
      "Epoch #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 71.2 +/- 65.9, min: -385.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 69.1 +/- 82.6, min: -385.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 23.57 minutes\n",
      "\n",
      "Epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 81.5 +/- 23.4, min: -310.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 84.6 +/- 17.0, min: 25.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 30.30 minutes\n",
      "\n",
      "Epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 82.1 +/- 21.9, min: -310.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 84.4 +/- 15.2, min: 47.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 37.00 minutes\n",
      "======================================\n",
      "Training finished. It's time to watch!\n",
      "Total score:  70.0\n",
      "Total score:  94.0\n",
      "Total score:  67.0\n",
      "Total score:  95.0\n",
      "Total score:  96.0\n",
      "Total score:  71.0\n",
      "Total score:  42.0\n",
      "Total score:  96.0\n",
      "Total score:  71.0\n",
      "Total score:  95.0\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from time import sleep, time\n",
    "\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "import vizdoom as vzd\n",
    "\n",
    "learning_rate = 0.00025\n",
    "discount_factor = 0.99\n",
    "train_epochs = 5\n",
    "learning_steps_per_epoch = 2000\n",
    "replay_memory_size = 10000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "test_episodes_per_epoch = 100\n",
    "\n",
    "frame_repeat = 12\n",
    "resolution = (30, 45)\n",
    "episodes_to_watch = 10\n",
    "\n",
    "model_savefile = \"./model-doom.pth\"\n",
    "save_model = True\n",
    "load_model = False\n",
    "skip_learning = False\n",
    "\n",
    "config_file_path = os.path.join(vzd.scenarios_path, \"simpler_basic.cfg\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"Down samples image to resolution\"\"\"\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_simple_game():\n",
    "    print(\"Initializing doom...\")\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(config_file_path)\n",
    "    game.set_window_visible(False)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "\n",
    "    return game\n",
    "\n",
    "\n",
    "def test(game, agent):\n",
    "    \"\"\"Runs a test_episodes_per_epoch episodes and prints the result\"\"\"\n",
    "    print(\"\\nTesting...\")\n",
    "    test_scores = []\n",
    "    for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.make_action(actions[best_action_index], frame_repeat)\n",
    "        r = game.get_total_reward()\n",
    "        test_scores.append(r)\n",
    "\n",
    "    test_scores = np.array(test_scores)\n",
    "    print(\n",
    "        \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
    "            test_scores.mean(), test_scores.std()\n",
    "        ),\n",
    "        \"min: %.1f\" % test_scores.min(),\n",
    "        \"max: %.1f\" % test_scores.max(),\n",
    "    )\n",
    "\n",
    "\n",
    "def run(game, agent, actions, num_epochs, frame_repeat, steps_per_epoch=2000):\n",
    "    \"\"\"\n",
    "    Run num epochs of training episodes.\n",
    "    Skip frame_repeat number of frames after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        game.new_episode()\n",
    "        train_scores = []\n",
    "        global_step = 0\n",
    "        print(f\"\\nEpoch #{epoch + 1}\")\n",
    "\n",
    "        for _ in trange(steps_per_epoch, leave=False):\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            action = agent.get_action(state)\n",
    "            reward = game.make_action(actions[action], frame_repeat)\n",
    "            done = game.is_episode_finished()\n",
    "\n",
    "            if not done:\n",
    "                next_state = preprocess(game.get_state().screen_buffer)\n",
    "            else:\n",
    "                next_state = np.zeros((1, 30, 45)).astype(np.float32)\n",
    "\n",
    "            agent.append_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            if global_step > agent.batch_size:\n",
    "                agent.train()\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(game.get_total_reward())\n",
    "                game.new_episode()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        agent.update_target_net()\n",
    "        train_scores = np.array(train_scores)\n",
    "\n",
    "        print(\n",
    "            \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
    "                train_scores.mean(), train_scores.std()\n",
    "            ),\n",
    "            \"min: %.1f,\" % train_scores.min(),\n",
    "            \"max: %.1f,\" % train_scores.max(),\n",
    "        )\n",
    "\n",
    "        test(game, agent)\n",
    "        if save_model:\n",
    "            print(\"Saving the network weights to:\", model_savefile)\n",
    "            torch.save(agent.q_net, model_savefile)\n",
    "        print(\"Total elapsed time: %.2f minutes\" % ((time() - start_time) / 60.0))\n",
    "\n",
    "    game.close()\n",
    "    return agent, game\n",
    "\n",
    "\n",
    "class DuelQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This is Duel DQN architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, available_actions_count):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.state_fc = nn.Sequential(nn.Linear(96, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(96, 64), nn.ReLU(), nn.Linear(64, available_actions_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(-1, 192)\n",
    "        x1 = x[:, :96]  \n",
    "        x2 = x[:, 96:]\n",
    "        state_value = self.state_fc(x1).reshape(-1, 1)\n",
    "        advantage_values = self.advantage_fc(x2)\n",
    "        x = state_value + (\n",
    "            advantage_values - advantage_values.mean(dim=1).reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size,\n",
    "        memory_size,\n",
    "        batch_size,\n",
    "        discount_factor,\n",
    "        lr,\n",
    "        load_model,\n",
    "        epsilon=1,\n",
    "        epsilon_decay=0.9996,\n",
    "        epsilon_min=0.1,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount_factor\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        if load_model:\n",
    "            print(\"Loading model from: \", model_savefile)\n",
    "            self.q_net = torch.load(model_savefile)\n",
    "            self.target_net = torch.load(model_savefile)\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing new model\")\n",
    "            self.q_net = DuelQNet(action_size).to(DEVICE)\n",
    "            self.target_net = DuelQNet(action_size).to(DEVICE)\n",
    "\n",
    "        self.opt = optim.SGD(self.q_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            state = torch.from_numpy(state).float().to(DEVICE)\n",
    "            action = torch.argmax(self.q_net(state)).item()\n",
    "            return action\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        batch = np.array(batch, dtype=object)\n",
    "\n",
    "        states = np.stack(batch[:, 0]).astype(float)\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2].astype(float)\n",
    "        next_states = np.stack(batch[:, 3]).astype(float)\n",
    "        dones = batch[:, 4].astype(bool)\n",
    "        not_dones = ~dones\n",
    "\n",
    "        row_idx = np.arange(self.batch_size)  \n",
    "\n",
    "        # value of the next states with double q learning\n",
    "        with torch.no_grad():\n",
    "            next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "            idx = row_idx, np.argmax(self.q_net(next_states).cpu().data.numpy(), 1)\n",
    "            next_state_values = self.target_net(next_states).cpu().data.numpy()[idx]\n",
    "            next_state_values = next_state_values[not_dones]\n",
    "\n",
    "        q_targets = rewards.copy()\n",
    "        q_targets[not_dones] += self.discount * next_state_values\n",
    "        q_targets = torch.from_numpy(q_targets).float().to(DEVICE)\n",
    "\n",
    "        idx = row_idx, actions\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "        action_values = self.q_net(states)[idx].float().to(DEVICE)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        td_error = self.criterion(q_targets, action_values)\n",
    "        td_error.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    game = create_simple_game()\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        len(actions),\n",
    "        lr=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        memory_size=replay_memory_size,\n",
    "        discount_factor=discount_factor,\n",
    "        load_model=load_model,\n",
    "    )\n",
    "\n",
    "    if not skip_learning:\n",
    "        agent, game = run(\n",
    "            game,\n",
    "            agent,\n",
    "            actions,\n",
    "            num_epochs=train_epochs,\n",
    "            frame_repeat=frame_repeat,\n",
    "            steps_per_epoch=learning_steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        print(\"======================================\")\n",
    "        print(\"Training finished. It's time to watch!\")\n",
    "\n",
    "    game.close()\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
    "    game.init()\n",
    "\n",
    "    for _ in range(episodes_to_watch):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.set_action(actions[best_action_index])\n",
    "            for _ in range(frame_repeat):\n",
    "                game.advance_action()\n",
    "\n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score:  95.0\n",
      "Total score:  83.0\n",
      "Total score:  71.0\n",
      "Total score:  95.0\n",
      "Total score:  94.0\n",
      "Total score:  -298.0\n",
      "Total score:  36.0\n",
      "Total score:  94.0\n",
      "Total score:  94.0\n",
      "Total score:  58.0\n"
     ]
    }
   ],
   "source": [
    "game.set_window_visible(True)\n",
    "game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
    "game.init()\n",
    "\n",
    "for _ in range(episodes_to_watch):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.set_action(actions[best_action_index])\n",
    "            for _ in range(frame_repeat):\n",
    "                game.advance_action()\n",
    "\n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
