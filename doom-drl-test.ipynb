{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: vizdoom[gym] in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (2.5.2)\n",
      "Requirement already satisfied: gym>=0.26.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from vizdoom[gym]) (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gym>=0.26.0->vizdoom[gym]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gym>=0.26.0->vizdoom[gym]) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom[gym]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom[gym]) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: stable-baselines3 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from stable-baselines3) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from matplotlib->stable-baselines3) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (4.10.0.82)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-image in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (2024.5.22)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (24.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mojitrk\\persistent\\dev\\doom-drl-agent\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 861.2 kB/s eta 0:00:00\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#prerequisite package installation\n",
    "%pip install vizdoom\n",
    "%pip install vizdoom[gym]\n",
    "%pip install gymnasium\n",
    "%pip install stable-baselines3 \n",
    "%pip install opencv-python\n",
    "#%pip uninstall gym \n",
    "#%pip uninstall shimmy \n",
    "#%pip uninstall tensorboard\n",
    "%pip install scikit-image\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import vizdoom\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Instantiate a VizDoom game instance.\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config('scenarios/basic.cfg')\n",
    "    game.init()\n",
    "\n",
    "    # Define possible actions. Each number represents the state of a button (1=active).\n",
    "    sample_actions = [\n",
    "        [1, 0, 0],  # Move left\n",
    "        [0, 1, 0],  # Move right\n",
    "        [0, 0, 1],  # Attack\n",
    "    ]\n",
    "\n",
    "    n_episodes = 10\n",
    "    current_episode = 0\n",
    "    \n",
    "    while current_episode < n_episodes:\n",
    "        game.make_action(random.choice(sample_actions))\n",
    "\n",
    "        if game.is_episode_finished():\n",
    "            current_episode += 1\n",
    "            game.new_episode()\n",
    "\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vizdoom as vzd\n",
    "\n",
    "#create vizdoom instance without gymnasium\n",
    "game = vzd.DoomGame()\n",
    "game.load_config(os.path.join(vzd.scenarios_path, \"deathmatch.cfg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from vizdoom import gymnasium_wrapper\n",
    "\n",
    "#create vizdoom instance with gymnasium\n",
    "env = gymnasium.make(\"VizdoomBasic-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "#####################################################################\n",
    "# This script presents how to use the most basic features of the environment.\n",
    "# It configures the engine, and makes the agent perform random actions.\n",
    "# It also gets current state and reward earned with the action.\n",
    "# <episodes> number of episodes are played.\n",
    "# Random combination of buttons is chosen for every action.\n",
    "# Game variables from state and last reward are printed.\n",
    "#\n",
    "# To see the scenario description go to \"../../scenarios/README.md\"\n",
    "#####################################################################\n",
    "\n",
    "import os\n",
    "from random import choice\n",
    "from time import sleep\n",
    "\n",
    "import vizdoom as vzd\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create DoomGame instance. It will run the game and communicate with you.\n",
    "    game = vzd.DoomGame()\n",
    "\n",
    "    # Now it's time for configuration!\n",
    "    # load_config could be used to load configuration instead of doing it here with code.\n",
    "    # If load_config is used in-code configuration will also work - most recent changes will add to previous ones.\n",
    "    # game.load_config(\"../../scenarios/basic.cfg\")\n",
    "\n",
    "    # Sets path to additional resources wad file which is basically your scenario wad.\n",
    "    # If not specified default maps will be used and it's pretty much useless... unless you want to play good old Doom.\n",
    "    game.set_doom_scenario_path(os.path.join(vzd.scenarios_path, \"basic.wad\"))\n",
    "\n",
    "    # Sets map to start (scenario .wad files can contain many maps).\n",
    "    game.set_doom_map(\"map01\")\n",
    "\n",
    "    # Sets resolution. Default is 320X240\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "\n",
    "    # Sets the screen buffer format. Not used here but now you can change it. Default is CRCGCB.\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "\n",
    "    # Enables depth buffer (turned off by default).\n",
    "    game.set_depth_buffer_enabled(True)\n",
    "\n",
    "    # Enables labeling of in-game objects labeling (turned off by default).\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "\n",
    "    # Enables buffer with a top-down map of the current episode/level (turned off by default).\n",
    "    game.set_automap_buffer_enabled(True)\n",
    "\n",
    "    # Enables information about all objects present in the current episode/level (turned off by default).\n",
    "    game.set_objects_info_enabled(True)\n",
    "\n",
    "    # Enables information about all sectors (map layout/geometry, turned off by default).\n",
    "    game.set_sectors_info_enabled(True)\n",
    "\n",
    "    # Sets other rendering options (all of these options except crosshair are enabled (set to True) by default)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False)  # If hud is enabled\n",
    "    game.set_render_crosshair(False)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)  # Bullet holes and blood on the walls\n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False)  # Like smoke and blood\n",
    "    game.set_render_messages(False)  # In-game text messages\n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(\n",
    "        True\n",
    "    )  # Effect upon taking damage or picking up items\n",
    "\n",
    "    # Adds buttons that will be allowed to use.\n",
    "    # This can be done by adding buttons one by one:\n",
    "    # game.clear_available_buttons()\n",
    "    # game.add_available_button(vzd.Button.MOVE_LEFT)\n",
    "    # game.add_available_button(vzd.Button.MOVE_RIGHT)\n",
    "    # game.add_available_button(vzd.Button.ATTACK)\n",
    "    # Or by setting them all at once:\n",
    "    game.set_available_buttons(\n",
    "        [vzd.Button.MOVE_LEFT, vzd.Button.MOVE_RIGHT, vzd.Button.ATTACK]\n",
    "    )\n",
    "    # Buttons that will be used can be also checked by:\n",
    "    print(\"Available buttons:\", [b.name for b in game.get_available_buttons()])\n",
    "\n",
    "    # Adds game variables that will be included in state.\n",
    "    # Similarly to buttons, they can be added one by one:\n",
    "    # game.clear_available_game_variables()\n",
    "    # game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "    # Or:\n",
    "    game.set_available_game_variables([vzd.GameVariable.AMMO2])\n",
    "    print(\n",
    "        \"Available game variables:\",\n",
    "        [v.name for v in game.get_available_game_variables()],\n",
    "    )\n",
    "\n",
    "    # Causes episodes to finish after 200 tics (actions)\n",
    "    game.set_episode_timeout(200)\n",
    "\n",
    "    # Makes episodes start after 10 tics (~after raising the weapon)\n",
    "    game.set_episode_start_time(10)\n",
    "\n",
    "    # Makes the window appear (turned on by default)\n",
    "    game.set_window_visible(True)\n",
    "\n",
    "    # Turns on the sound. (turned off by default)\n",
    "    # game.set_sound_enabled(True)\n",
    "    # Because of some problems with OpenAL on Ubuntu 20.04, we keep this line commented,\n",
    "    # the sound is only useful for humans watching the game.\n",
    "\n",
    "    # Turns on the audio buffer. (turned off by default)\n",
    "    # If this is switched on, the audio will stop playing on device, even with game.set_sound_enabled(True)\n",
    "    # Setting game.set_sound_enabled(True) is not required for audio buffer to work.\n",
    "    # game.set_audio_buffer_enabled(True)\n",
    "    # Because of some problems with OpenAL on Ubuntu 20.04, we keep this line commented.\n",
    "\n",
    "    # Sets the living reward (for each move) to -1\n",
    "    game.set_living_reward(-1)\n",
    "\n",
    "    # Sets ViZDoom mode (PLAYER, ASYNC_PLAYER, SPECTATOR, ASYNC_SPECTATOR, PLAYER mode is default)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "\n",
    "    # Enables engine output to console, in case of a problem this might provide additional information.\n",
    "    # game.set_console_enabled(True)\n",
    "\n",
    "    # Initialize the game. Further configuration won't take any effect from now on.\n",
    "    game.init()\n",
    "\n",
    "    # Define some actions. Each list entry corresponds to declared buttons:\n",
    "    # MOVE_LEFT, MOVE_RIGHT, ATTACK\n",
    "    # game.get_available_buttons_size() can be used to check the number of available buttons.\n",
    "    # 5 more combinations are naturally possible but only 3 are included for transparency when watching.\n",
    "    actions = [[True, False, False], [False, True, False], [False, False, True]]\n",
    "\n",
    "    # Run this many episodes\n",
    "    episodes = 10\n",
    "\n",
    "    # Sets time that will pause the engine after each action (in seconds)\n",
    "    # Without this everything would go too fast for you to keep track of what's happening.\n",
    "    sleep_time = 1.0 / vzd.DEFAULT_TICRATE  # = 0.028\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(f\"Episode #{i + 1}\")\n",
    "\n",
    "        # Starts a new episode. It is not needed right after init() but it doesn't cost much. At least the loop is nicer.\n",
    "        game.new_episode()\n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "\n",
    "            # Gets the state\n",
    "            state = game.get_state()\n",
    "\n",
    "            # Which consists of:\n",
    "            n = state.number\n",
    "            vars = state.game_variables\n",
    "\n",
    "            # Different buffers (screens, depth, labels, automap, audio)\n",
    "            # Expect of screen buffer some may be None if not first enabled.\n",
    "            screen_buf = state.screen_buffer\n",
    "            depth_buf = state.depth_buffer\n",
    "            labels_buf = state.labels_buffer\n",
    "            automap_buf = state.automap_buffer\n",
    "            audio_buf = state.audio_buffer\n",
    "\n",
    "            # List of labeled objects visible in the frame, may be None if not first enabled.\n",
    "            labels = state.labels\n",
    "\n",
    "            # List of all objects (enemies, pickups, etc.) present in the current episode, may be None if not first enabled\n",
    "            objects = state.objects\n",
    "\n",
    "            # List of all sectors (map geometry), may be None if not first enabled.\n",
    "            sectors = state.sectors\n",
    "\n",
    "            # Games variables can be also accessed via\n",
    "            # (including the ones that were not added as available to a game state):\n",
    "            # game.get_game_variable(GameVariable.AMMO2)\n",
    "\n",
    "            # Makes an action (here random one) and returns a reward.\n",
    "            r = game.make_action(choice(actions))\n",
    "\n",
    "            # Makes a \"prolonged\" action and skip frames:\n",
    "            # skiprate = 4\n",
    "            # r = game.make_action(choice(actions), skiprate)\n",
    "\n",
    "            # The same could be achieved with:\n",
    "            # game.set_action(choice(actions))\n",
    "            # game.advance_action(skiprate)\n",
    "            # r = game.get_last_reward()\n",
    "\n",
    "            # Prints state's game variables and reward.\n",
    "            print(f\"State #{n}\")\n",
    "            print(\"Game variables:\", vars)\n",
    "            print(\"Reward:\", r)\n",
    "            print(\"=====================\")\n",
    "\n",
    "            if sleep_time > 0:\n",
    "                sleep(sleep_time)\n",
    "\n",
    "        # Check how the episode went.\n",
    "        print(\"Episode finished.\")\n",
    "        print(\"Total reward:\", game.get_total_reward())\n",
    "        print(\"************************\")\n",
    "\n",
    "    # It will be done automatically anyway but sometimes you need to do it in the middle of the program...\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# This script presents how to use Doom's native demo mechanism to\n",
    "# replay episodes with perfect accuracy.\n",
    "#####################################################################\n",
    "\n",
    "import os\n",
    "from random import choice\n",
    "\n",
    "import vizdoom as vzd\n",
    "\n",
    "\n",
    "game = vzd.DoomGame()\n",
    "\n",
    "# Use other config file if you wish.\n",
    "game.load_config(os.path.join(vzd.scenarios_path, \"basic.cfg\"))\n",
    "game.set_episode_timeout(100)\n",
    "\n",
    "# Record episodes while playing in 320x240 resolution without HUD\n",
    "game.set_screen_resolution(vzd.ScreenResolution.RES_320X240)\n",
    "game.set_render_hud(False)\n",
    "\n",
    "# Episodes can be recorder in any available mode (PLAYER, ASYNC_PLAYER, SPECTATOR, ASYNC_SPECTATOR)\n",
    "game.set_mode(vzd.Mode.PLAYER)\n",
    "\n",
    "game.init()\n",
    "\n",
    "actions = [[True, False, False], [False, True, False], [False, False, True]]\n",
    "\n",
    "# Run and record this many episodes\n",
    "episodes = 2\n",
    "\n",
    "# Recording\n",
    "print(\"\\nRECORDING EPISODES\")\n",
    "print(\"************************\\n\")\n",
    "\n",
    "for i in range(episodes):\n",
    "\n",
    "    # new_episode can record the episode using Doom's demo recording functionality to given file.\n",
    "    # Recorded episodes can be reconstructed with perfect accuracy using different rendering settings.\n",
    "    # This can not be used to record episodes in multiplayer mode.\n",
    "    game.new_episode(f\"episode{i}_rec.lmp\")\n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "        s = game.get_state()\n",
    "\n",
    "        a = choice(actions)\n",
    "        r = game.make_action(choice(actions))\n",
    "\n",
    "        print(f\"State #{s.number}\")\n",
    "        print(\"Action:\", a)\n",
    "        print(\"Game variables:\", s.game_variables[0])\n",
    "        print(\"Reward:\", r)\n",
    "        print(\"=====================\")\n",
    "\n",
    "    print(f\"Episode {i} finished. Saved to file episode{i}_rec.lmp\")\n",
    "    print(\"Total reward:\", game.get_total_reward())\n",
    "    print(\"************************\\n\")\n",
    "\n",
    "game.new_episode()  # This is currently required to stop and save the previous recording.\n",
    "game.close()\n",
    "\n",
    "# New render settings for replay\n",
    "game.set_screen_resolution(vzd.ScreenResolution.RES_800X600)\n",
    "game.set_render_hud(True)\n",
    "\n",
    "# Replay can be played in any mode.\n",
    "game.set_mode(vzd.Mode.SPECTATOR)\n",
    "\n",
    "game.init()\n",
    "\n",
    "print(\"\\nREPLAY OF EPISODE\")\n",
    "print(\"************************\\n\")\n",
    "\n",
    "for i in range(episodes):\n",
    "\n",
    "    # Replays episodes stored in given file. Sending game command will interrupt playback.\n",
    "    game.replay_episode(f\"episode{i}_rec.lmp\")\n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "        # Get a state\n",
    "        s = game.get_state()\n",
    "\n",
    "        # Use advance_action instead of make_action to proceed\n",
    "        game.advance_action()\n",
    "\n",
    "        # Retrieve the last actions and the reward\n",
    "        a = game.get_last_action()\n",
    "        r = game.get_last_reward()\n",
    "\n",
    "        print(f\"State #{s.number}\")\n",
    "        print(\"Action:\", a)\n",
    "        print(\"Game variables:\", s.game_variables[0])\n",
    "        print(\"Reward:\", r)\n",
    "        print(\"=====================\")\n",
    "\n",
    "    print(\"Episode\", i, \"finished.\")\n",
    "    print(\"Total reward:\", game.get_total_reward())\n",
    "    print(\"************************\")\n",
    "\n",
    "game.close()\n",
    "\n",
    "# Delete recordings (*.lmp files).\n",
    "for i in range(episodes):\n",
    "    os.remove(f\"episode{i}_rec.lmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CartPole-v0': EnvSpec(id='CartPole-v0', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=0),\n",
       " 'CartPole-v1': EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1),\n",
       " 'MountainCar-v0': EnvSpec(id='MountainCar-v0', entry_point='gym.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCar', version=0),\n",
       " 'MountainCarContinuous-v0': EnvSpec(id='MountainCarContinuous-v0', entry_point='gym.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0),\n",
       " 'Pendulum-v1': EnvSpec(id='Pendulum-v1', entry_point='gym.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pendulum', version=1),\n",
       " 'Acrobot-v1': EnvSpec(id='Acrobot-v1', entry_point='gym.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Acrobot', version=1),\n",
       " 'LunarLander-v2': EnvSpec(id='LunarLander-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='LunarLander', version=2),\n",
       " 'LunarLanderContinuous-v2': EnvSpec(id='LunarLanderContinuous-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2),\n",
       " 'BipedalWalker-v3': EnvSpec(id='BipedalWalker-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='BipedalWalker', version=3),\n",
       " 'BipedalWalkerHardcore-v3': EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3),\n",
       " 'CarRacing-v2': EnvSpec(id='CarRacing-v2', entry_point='gym.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CarRacing', version=2),\n",
       " 'Blackjack-v1': EnvSpec(id='Blackjack-v1', entry_point='gym.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1),\n",
       " 'FrozenLake-v1': EnvSpec(id='FrozenLake-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1),\n",
       " 'FrozenLake8x8-v1': EnvSpec(id='FrozenLake8x8-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1),\n",
       " 'CliffWalking-v0': EnvSpec(id='CliffWalking-v0', entry_point='gym.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CliffWalking', version=0),\n",
       " 'Taxi-v3': EnvSpec(id='Taxi-v3', entry_point='gym.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Taxi', version=3),\n",
       " 'Reacher-v2': EnvSpec(id='Reacher-v2', entry_point='gym.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=2),\n",
       " 'Reacher-v4': EnvSpec(id='Reacher-v4', entry_point='gym.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=4),\n",
       " 'Pusher-v2': EnvSpec(id='Pusher-v2', entry_point='gym.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=2),\n",
       " 'Pusher-v4': EnvSpec(id='Pusher-v4', entry_point='gym.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=4),\n",
       " 'InvertedPendulum-v2': EnvSpec(id='InvertedPendulum-v2', entry_point='gym.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2),\n",
       " 'InvertedPendulum-v4': EnvSpec(id='InvertedPendulum-v4', entry_point='gym.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4),\n",
       " 'InvertedDoublePendulum-v2': EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gym.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2),\n",
       " 'InvertedDoublePendulum-v4': EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gym.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4),\n",
       " 'HalfCheetah-v2': EnvSpec(id='HalfCheetah-v2', entry_point='gym.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=2),\n",
       " 'HalfCheetah-v3': EnvSpec(id='HalfCheetah-v3', entry_point='gym.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=3),\n",
       " 'HalfCheetah-v4': EnvSpec(id='HalfCheetah-v4', entry_point='gym.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=4),\n",
       " 'Hopper-v2': EnvSpec(id='Hopper-v2', entry_point='gym.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=2),\n",
       " 'Hopper-v3': EnvSpec(id='Hopper-v3', entry_point='gym.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=3),\n",
       " 'Hopper-v4': EnvSpec(id='Hopper-v4', entry_point='gym.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=4),\n",
       " 'Swimmer-v2': EnvSpec(id='Swimmer-v2', entry_point='gym.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=2),\n",
       " 'Swimmer-v3': EnvSpec(id='Swimmer-v3', entry_point='gym.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=3),\n",
       " 'Swimmer-v4': EnvSpec(id='Swimmer-v4', entry_point='gym.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=4),\n",
       " 'Walker2d-v2': EnvSpec(id='Walker2d-v2', entry_point='gym.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=2),\n",
       " 'Walker2d-v3': EnvSpec(id='Walker2d-v3', entry_point='gym.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=3),\n",
       " 'Walker2d-v4': EnvSpec(id='Walker2d-v4', entry_point='gym.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=4),\n",
       " 'Ant-v2': EnvSpec(id='Ant-v2', entry_point='gym.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=2),\n",
       " 'Ant-v3': EnvSpec(id='Ant-v3', entry_point='gym.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=3),\n",
       " 'Ant-v4': EnvSpec(id='Ant-v4', entry_point='gym.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=4),\n",
       " 'Humanoid-v2': EnvSpec(id='Humanoid-v2', entry_point='gym.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=2),\n",
       " 'Humanoid-v3': EnvSpec(id='Humanoid-v3', entry_point='gym.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=3),\n",
       " 'Humanoid-v4': EnvSpec(id='Humanoid-v4', entry_point='gym.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=4),\n",
       " 'HumanoidStandup-v2': EnvSpec(id='HumanoidStandup-v2', entry_point='gym.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2),\n",
       " 'HumanoidStandup-v4': EnvSpec(id='HumanoidStandup-v4', entry_point='gym.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4),\n",
       " 'VizdoomBasic-v0': EnvSpec(id='VizdoomBasic-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'basic.cfg'}, namespace=None, name='VizdoomBasic', version=0),\n",
       " 'VizdoomCorridor-v0': EnvSpec(id='VizdoomCorridor-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'deadly_corridor.cfg'}, namespace=None, name='VizdoomCorridor', version=0),\n",
       " 'VizdoomDefendCenter-v0': EnvSpec(id='VizdoomDefendCenter-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'defend_the_center.cfg'}, namespace=None, name='VizdoomDefendCenter', version=0),\n",
       " 'VizdoomDefendLine-v0': EnvSpec(id='VizdoomDefendLine-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'defend_the_line.cfg'}, namespace=None, name='VizdoomDefendLine', version=0),\n",
       " 'VizdoomHealthGathering-v0': EnvSpec(id='VizdoomHealthGathering-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'health_gathering.cfg'}, namespace=None, name='VizdoomHealthGathering', version=0),\n",
       " 'VizdoomMyWayHome-v0': EnvSpec(id='VizdoomMyWayHome-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'my_way_home.cfg'}, namespace=None, name='VizdoomMyWayHome', version=0),\n",
       " 'VizdoomPredictPosition-v0': EnvSpec(id='VizdoomPredictPosition-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'predict_position.cfg'}, namespace=None, name='VizdoomPredictPosition', version=0),\n",
       " 'VizdoomTakeCover-v0': EnvSpec(id='VizdoomTakeCover-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'take_cover.cfg'}, namespace=None, name='VizdoomTakeCover', version=0),\n",
       " 'VizdoomDeathmatch-v0': EnvSpec(id='VizdoomDeathmatch-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'deathmatch.cfg'}, namespace=None, name='VizdoomDeathmatch', version=0),\n",
       " 'VizdoomHealthGatheringSupreme-v0': EnvSpec(id='VizdoomHealthGatheringSupreme-v0', entry_point='vizdoom.gym_wrapper.gym_env_defns:VizdoomScenarioEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'scenario_file': 'health_gathering_supreme.cfg'}, namespace=None, name='VizdoomHealthGatheringSupreme', version=0)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Train stable-baselines3 PPO agents on ViZDoom. [-h]\n",
      "                                                      [--env {R,e,a,c,h,e,r,-,v,2}]\n",
      "Train stable-baselines3 PPO agents on ViZDoom.: error: unrecognized arguments: --f=c:\\Users\\Mojitrk\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-4516CY2smzX2hvj7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Example script of training agents with stable-baselines3\n",
    "# on ViZDoom using the Gym API\n",
    "#\n",
    "# Note: ViZDoom must be installed with optional gym dependencies:\n",
    "#         pip install vizdoom[gym]\n",
    "#       You also need stable-baselines3:\n",
    "#         pip install stable-baselines3\n",
    "#\n",
    "# See more stable-baselines3 documentation here:\n",
    "#   https://stable-baselines3.readthedocs.io/en/master/index.html\n",
    "#####################################################################\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gym\n",
    "import vizdoom.gym_wrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "DEFAULT_ENV = \"VizdoomBasic-v0\"\n",
    "#AVAILABLE_ENVS = [env for env in [env_spec.id for env_spec in gym.envs.registry.all()] if \"Vizdoom\" in env]\n",
    "#AVAILABLE_ENVS = [env for env in [env_spec.index(id) for env_spec in gym.envs.registry] if \"Vizdoom\" in env]\n",
    "# Height and width of the resized image\n",
    "IMAGE_SHAPE = (60, 80)\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_TIMESTEPS = int(1e6)\n",
    "N_STEPS = 128\n",
    "N_ENVS = 8\n",
    "FRAME_SKIP = 4\n",
    "\n",
    "\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    ViZDoom environments return dictionaries as observations, containing\n",
    "    the main image as well other info.\n",
    "    The image is also too large for normal training.\n",
    "\n",
    "    This wrapper replaces the dictionary observation space with a simple\n",
    "    Box space (i.e., only the RGB image), and also resizes the image to a\n",
    "    smaller size.\n",
    "\n",
    "    NOTE: Ideally, you should set the image size to smaller in the scenario files\n",
    "          for faster running of ViZDoom. This can really impact performance,\n",
    "          and this code is pretty slow because of this!\n",
    "    \"\"\"\n",
    "    def __init__(self, env, shape=IMAGE_SHAPE):\n",
    "        super().__init__(env)\n",
    "        self.image_shape = shape\n",
    "        self.image_shape_reverse = shape[::-1]\n",
    "\n",
    "        # Create new observation space with the new shape\n",
    "        num_channels = env.observation_space[\"rgb\"].shape[-1]\n",
    "        new_shape = (shape[0], shape[1], num_channels)\n",
    "        self.observation_space = gym.spaces.Box(0, 255, shape=new_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.resize(observation[\"rgb\"], self.image_shape_reverse)\n",
    "        return observation\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Create multiple environments: this speeds up training with PPO\n",
    "    # We apply two wrappers on the environment:\n",
    "    #  1) The above wrapper that modifies the observations (takes only the image and resizes it)\n",
    "    #  2) A reward scaling wrapper. Normally the scenarios use large magnitudes for rewards (e.g., 100, -100).\n",
    "    #     This may lead to unstable learning, and we scale the rewards by 1/100\n",
    "    def wrap_env(env):\n",
    "        env = ObservationWrapper(env)\n",
    "        env = gym.wrappers.TransformReward(env, lambda r: r * 0.01)\n",
    "        return env\n",
    "\n",
    "    envs = make_vec_env(\n",
    "        args.env,\n",
    "        n_envs=N_ENVS,\n",
    "        wrapper_class=wrap_env\n",
    "    )\n",
    "\n",
    "    agent = PPO(\"CnnPolicy\", envs, n_steps=N_STEPS, verbose=1)\n",
    "\n",
    "    # Do the actual learning\n",
    "    # This will print out the results in the console.\n",
    "    # If agent gets better, \"ep_rew_mean\" should increase steadily\n",
    "    agent.learn(total_timesteps=TRAINING_TIMESTEPS)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser(\"Train stable-baselines3 PPO agents on ViZDoom.\")\n",
    "    parser.add_argument(\"--env\",\n",
    "                        default=DEFAULT_ENV,\n",
    "                        #choices=AVAILABLE_ENVS,\n",
    "                        choices='Reacher-v2',\n",
    "                        help=\"Name of the environment to play\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 98\u001b[0m\n\u001b[0;32m     92\u001b[0m parser \u001b[38;5;241m=\u001b[39m ArgumentParser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain stable-baselines3 PPO agents on ViZDoom.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--env\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m                     default\u001b[38;5;241m=\u001b[39mDEFAULT_ENV,\n\u001b[0;32m     95\u001b[0m                     \u001b[38;5;66;03m#choices=AVAILABLE_ENVS,\u001b[39;00m\n\u001b[0;32m     96\u001b[0m                     choices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReacher-v2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     97\u001b[0m                     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of the environment to play\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m main(args)\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\argparse.py:1877\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[0;32m   1876\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1877\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\argparse.py:2640\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m   2639\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\argparse.py:2627\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[0;32m   2626\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2627\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n",
      "Initializing new model\n",
      "\n",
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -53.8 +/- 178.5, min: -385.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 3.9 +/- 155.9, min: -375.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 6.27 minutes\n",
      "\n",
      "Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 47.0 +/- 101.9, min: -385.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 66.8 +/- 79.8, min: -370.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 12.41 minutes\n",
      "\n",
      "Epoch #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 74.0 +/- 56.9, min: -400.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 70.8 +/- 83.7, min: -400.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 21.34 minutes\n",
      "\n",
      "Epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 77.9 +/- 26.5, min: -305.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 74.8 +/- 18.9, min: -9.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 30.02 minutes\n",
      "\n",
      "Epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 76.5 +/- 21.8, min: -69.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 79.6 +/- 22.9, min: 1.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 39.32 minutes\n",
      "======================================\n",
      "Training finished. It's time to watch!\n",
      "Total score:  -4.0\n",
      "Total score:  94.0\n",
      "Total score:  94.0\n",
      "Total score:  82.0\n",
      "Total score:  94.0\n",
      "Total score:  18.0\n",
      "Total score:  82.0\n",
      "Total score:  82.0\n",
      "Total score:  94.0\n",
      "Total score:  94.0\n"
     ]
    }
   ],
   "source": [
    "# E. Culurciello, L. Mueller, Z. Boztoprak\n",
    "# December 2020\n",
    "\n",
    "import itertools as it\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from time import sleep, time\n",
    "\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "import vizdoom as vzd\n",
    "\n",
    "\n",
    "# Q-learning settings\n",
    "learning_rate = 0.00025\n",
    "discount_factor = 0.99\n",
    "train_epochs = 5\n",
    "learning_steps_per_epoch = 2000\n",
    "replay_memory_size = 10000\n",
    "\n",
    "# NN learning settings\n",
    "batch_size = 64\n",
    "\n",
    "# Training regime\n",
    "test_episodes_per_epoch = 100\n",
    "\n",
    "# Other parameters\n",
    "frame_repeat = 12\n",
    "resolution = (30, 45)\n",
    "episodes_to_watch = 10\n",
    "\n",
    "model_savefile = \"./model-doom.pth\"\n",
    "save_model = True\n",
    "load_model = False\n",
    "skip_learning = False\n",
    "\n",
    "# Configuration file path\n",
    "config_file_path = os.path.join(vzd.scenarios_path, \"simpler_basic.cfg\")\n",
    "# config_file_path = os.path.join(vzd.scenarios_path, \"rocket_basic.cfg\")\n",
    "# config_file_path = os.path.join(vzd.scenarios_path, \"basic.cfg\")\n",
    "\n",
    "# Uses GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"Down samples image to resolution\"\"\"\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_simple_game():\n",
    "    print(\"Initializing doom...\")\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(config_file_path)\n",
    "    game.set_window_visible(False)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "\n",
    "    return game\n",
    "\n",
    "\n",
    "def test(game, agent):\n",
    "    \"\"\"Runs a test_episodes_per_epoch episodes and prints the result\"\"\"\n",
    "    print(\"\\nTesting...\")\n",
    "    test_scores = []\n",
    "    for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.make_action(actions[best_action_index], frame_repeat)\n",
    "        r = game.get_total_reward()\n",
    "        test_scores.append(r)\n",
    "\n",
    "    test_scores = np.array(test_scores)\n",
    "    print(\n",
    "        \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
    "            test_scores.mean(), test_scores.std()\n",
    "        ),\n",
    "        \"min: %.1f\" % test_scores.min(),\n",
    "        \"max: %.1f\" % test_scores.max(),\n",
    "    )\n",
    "\n",
    "\n",
    "def run(game, agent, actions, num_epochs, frame_repeat, steps_per_epoch=2000):\n",
    "    \"\"\"\n",
    "    Run num epochs of training episodes.\n",
    "    Skip frame_repeat number of frames after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        game.new_episode()\n",
    "        train_scores = []\n",
    "        global_step = 0\n",
    "        print(f\"\\nEpoch #{epoch + 1}\")\n",
    "\n",
    "        for _ in trange(steps_per_epoch, leave=False):\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            action = agent.get_action(state)\n",
    "            reward = game.make_action(actions[action], frame_repeat)\n",
    "            done = game.is_episode_finished()\n",
    "\n",
    "            if not done:\n",
    "                next_state = preprocess(game.get_state().screen_buffer)\n",
    "            else:\n",
    "                next_state = np.zeros((1, 30, 45)).astype(np.float32)\n",
    "\n",
    "            agent.append_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            if global_step > agent.batch_size:\n",
    "                agent.train()\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(game.get_total_reward())\n",
    "                game.new_episode()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        agent.update_target_net()\n",
    "        train_scores = np.array(train_scores)\n",
    "\n",
    "        print(\n",
    "            \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
    "                train_scores.mean(), train_scores.std()\n",
    "            ),\n",
    "            \"min: %.1f,\" % train_scores.min(),\n",
    "            \"max: %.1f,\" % train_scores.max(),\n",
    "        )\n",
    "\n",
    "        test(game, agent)\n",
    "        if save_model:\n",
    "            print(\"Saving the network weights to:\", model_savefile)\n",
    "            torch.save(agent.q_net, model_savefile)\n",
    "        print(\"Total elapsed time: %.2f minutes\" % ((time() - start_time) / 60.0))\n",
    "\n",
    "    game.close()\n",
    "    return agent, game\n",
    "\n",
    "\n",
    "class DuelQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This is Duel DQN architecture.\n",
    "    see https://arxiv.org/abs/1511.06581 for more information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, available_actions_count):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.state_fc = nn.Sequential(nn.Linear(96, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(96, 64), nn.ReLU(), nn.Linear(64, available_actions_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(-1, 192)\n",
    "        x1 = x[:, :96]  # input for the net to calculate the state value\n",
    "        x2 = x[:, 96:]  # relative advantage of actions in the state\n",
    "        state_value = self.state_fc(x1).reshape(-1, 1)\n",
    "        advantage_values = self.advantage_fc(x2)\n",
    "        x = state_value + (\n",
    "            advantage_values - advantage_values.mean(dim=1).reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size,\n",
    "        memory_size,\n",
    "        batch_size,\n",
    "        discount_factor,\n",
    "        lr,\n",
    "        load_model,\n",
    "        epsilon=1,\n",
    "        epsilon_decay=0.9996,\n",
    "        epsilon_min=0.1,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount_factor\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        if load_model:\n",
    "            print(\"Loading model from: \", model_savefile)\n",
    "            self.q_net = torch.load(model_savefile)\n",
    "            self.target_net = torch.load(model_savefile)\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing new model\")\n",
    "            self.q_net = DuelQNet(action_size).to(DEVICE)\n",
    "            self.target_net = DuelQNet(action_size).to(DEVICE)\n",
    "\n",
    "        self.opt = optim.SGD(self.q_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            state = torch.from_numpy(state).float().to(DEVICE)\n",
    "            action = torch.argmax(self.q_net(state)).item()\n",
    "            return action\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        batch = np.array(batch, dtype=object)\n",
    "\n",
    "        states = np.stack(batch[:, 0]).astype(float)\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2].astype(float)\n",
    "        next_states = np.stack(batch[:, 3]).astype(float)\n",
    "        dones = batch[:, 4].astype(bool)\n",
    "        not_dones = ~dones\n",
    "\n",
    "        row_idx = np.arange(self.batch_size)  # used for indexing the batch\n",
    "\n",
    "        # value of the next states with double q learning\n",
    "        # see https://arxiv.org/abs/1509.06461 for more information on double q learning\n",
    "        with torch.no_grad():\n",
    "            next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "            idx = row_idx, np.argmax(self.q_net(next_states).cpu().data.numpy(), 1)\n",
    "            next_state_values = self.target_net(next_states).cpu().data.numpy()[idx]\n",
    "            next_state_values = next_state_values[not_dones]\n",
    "\n",
    "        # this defines y = r + discount * max_a q(s', a)\n",
    "        q_targets = rewards.copy()\n",
    "        q_targets[not_dones] += self.discount * next_state_values\n",
    "        q_targets = torch.from_numpy(q_targets).float().to(DEVICE)\n",
    "\n",
    "        # this selects only the q values of the actions taken\n",
    "        idx = row_idx, actions\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "        action_values = self.q_net(states)[idx].float().to(DEVICE)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        td_error = self.criterion(q_targets, action_values)\n",
    "        td_error.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize game and actions\n",
    "    game = create_simple_game()\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "    # Initialize our agent with the set parameters\n",
    "    agent = DQNAgent(\n",
    "        len(actions),\n",
    "        lr=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        memory_size=replay_memory_size,\n",
    "        discount_factor=discount_factor,\n",
    "        load_model=load_model,\n",
    "    )\n",
    "\n",
    "    # Run the training for the set number of epochs\n",
    "    if not skip_learning:\n",
    "        agent, game = run(\n",
    "            game,\n",
    "            agent,\n",
    "            actions,\n",
    "            num_epochs=train_epochs,\n",
    "            frame_repeat=frame_repeat,\n",
    "            steps_per_epoch=learning_steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        print(\"======================================\")\n",
    "        print(\"Training finished. It's time to watch!\")\n",
    "\n",
    "    # Reinitialize the game with window visible\n",
    "    game.close()\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
    "    game.init()\n",
    "\n",
    "    for _ in range(episodes_to_watch):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
    "            game.set_action(actions[best_action_index])\n",
    "            for _ in range(frame_repeat):\n",
    "                game.advance_action()\n",
    "\n",
    "        # Sleep between episodes\n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
