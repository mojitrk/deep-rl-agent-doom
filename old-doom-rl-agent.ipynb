{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import vizdoom\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common import callbacks\n",
    "from stable_baselines3.common import evaluation\n",
    "from stable_baselines3.common import policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the learning task, we will need two components: the agent and the environment. Thanks to stable-baselines, building an agent will be very straightforward. The environment though requires some explanation. In the medium article we explain how to build the Gym environment wrapper for a VizDoom game instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'envs' from 'common' (c:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\common\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m envs\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_env\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m envs\u001b[38;5;241m.\u001b[39mDoomEnv:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Create a VizDoom instance.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     game \u001b[38;5;241m=\u001b[39m vizdoom\u001b[38;5;241m.\u001b[39mDoomGame()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'envs' from 'common' (c:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\common\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from common import envs\n",
    "\n",
    "def create_env(**kwargs) -> envs.DoomEnv:\n",
    "    # Create a VizDoom instance.\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config('scenarios/basic.cfg')\n",
    "    game.init()\n",
    "\n",
    "    # Wrap the environment with the Gym adapter.\n",
    "    return envs.DoomEnv(game, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one additional step required to make it work with stable-baselines. The good new is that this small additional step will unlock very cools features for the learning task.\n",
    "\n",
    "What we need to set up is a vectorized environment. Stable-baselines' [doc](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vectransposeimage) has a nice explanation on what you can do with vectorized environments. Technically, we could do whithout but since we'll be exploring training on multiple environment later anyway, let's go ahead and see how to create one. In PyTorch, convolutional layers expect as inputs an image of the shape (C, H, W) that it, channels first. We could ask VizDoom to directly render the frame buffer in [channel-first](https://github.com/mwydmuch/ViZDoom/blob/master/doc/Types.md#-screenformat) mode. However, as we will see below, it is a better idea to keep the image channel last as this will allow us to make some image preprocessing using OpenCV. Thankfully, stable-baselines offers a [VecTransposeImage](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vectransposeimage) wrapper whose sole purpose is to convert from channel last to channel first and vice-versa. As you can see from the function signature, VecTransposeImage requires a VecEnv to work. So let's make one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "\n",
    "def create_vec_env(**kwargs) -> VecTransposeImage:\n",
    "    vec_env = DummyVecEnv([lambda: create_env(**kwargs)])\n",
    "    return VecTransposeImage(vec_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's unpack what's going on here. First we have create a dummy vectorized environment. It need a list of function that when called, will generate our environment. Since there is only one element in this list we will end up with only one environment. Learning with multiple environment later will be as simple as adding more elements to that list. Then, we pass this dummy vectorized environment to the image transposition wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although VizDoom provides many different screen resolutions. It is useful to define an additional frame processing step in which we rescale the input image by a given factor. This allows us the render the game in a nice resolution while keeping the input size to the model to an acceptable range. This can be easily done with a call to the resize function of OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_processor = lambda frame: cv2.resize(frame, None, fx=.5, fy=.5, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so allow us to take advantage of OpenCV's interpolation to have a smoother image at the same resolution. A side-by-side comparison should make the advantage of rescaling the image ourselves pretty clear. Notice how aliased the image on the left is. This effect get worse and worse as the game tries to render furhter away (smaller) objects. If we can \"see\" better in the second image, so will our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'basic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m      3\u001b[0m f, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Compare 160 by 120 resolution vs. 320 by 240 resolution, scaled by half using OpenCV.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'basic'"
     ]
    }
   ],
   "source": [
    "from basic import utils\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Compare 160 by 120 resolution vs. 320 by 240 resolution, scaled by half using OpenCV.\n",
    "ax[0].imshow(utils.get_sample_frame(vizdoom.ScreenResolution.RES_160X120))\n",
    "ax[1].imshow(frame_processor(utils.get_sample_frame(vizdoom.ScreenResolution.RES_320X240)))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the agent and the environment\n",
    "\n",
    "We will create two environment, one for training and one for evaluatinng our agent. We will pass to both those environment a couple of parameters that are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_env() missing 1 required positional argument: 'scenario'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m env_args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Number of steps for which the last action is repeated.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_skip\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Scaling the input image allows us to see the game in good resolution while \u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_processor\u001b[39m\u001b[38;5;124m'\u001b[39m: frame_processor\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create training and evaluation environments.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m training_env, eval_env \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vec_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_args\u001b[49m\u001b[43m)\u001b[49m, create_vec_env(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_args)\n",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mcreate_vec_env\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_vec_env\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecTransposeImage:\n\u001b[1;32m----> 4\u001b[0m     vec_env \u001b[38;5;241m=\u001b[39m \u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VecTransposeImage(vec_env)\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:30\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[1;34m(self, env_fns)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_patch_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_fns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mcreate_vec_env.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_vec_env\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecTransposeImage:\n\u001b[1;32m----> 4\u001b[0m     vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mcreate_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VecTransposeImage(vec_env)\n",
      "\u001b[1;31mTypeError\u001b[0m: create_env() missing 1 required positional argument: 'scenario'"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    # Number of steps for which the last action is repeated.\n",
    "    'frame_skip': 4,\n",
    "    # Scaling the input image allows us to see the game in good resolution while \n",
    "    'frame_processor': frame_processor\n",
    "}\n",
    "\n",
    "# Create training and evaluation environments.\n",
    "training_env, eval_env = create_vec_env(**env_args), create_vec_env(**env_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, with stable baselines creating the agent is really straightforward. We can leave most of the parameters to their default values. By calling the PPO constructor we will get an agent with an actor-critic policy with the following structure in five parts:\n",
    "\n",
    "```\n",
    "       features extractor\n",
    "               |\n",
    "          mlp extractor\n",
    "               |\n",
    "            features\n",
    "            /      \\\n",
    "           /        \\\n",
    "   action net      value net\n",
    "```\n",
    "If you want to know more about actor-critic methods, Rudy Gilman has an [awesome article](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752) vulgarizing the subject by means of a comic.\n",
    "\n",
    "By default, the feature extractor is the [NatureCNN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), the mlp extractor is empty and the action and value net are two sequential models mapping the features to the action and value output respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=11264, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=11264, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=11264, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_agent(env, **kwargs):\n",
    "    return ppo.PPO(policy=policies.ActorCriticCnnPolicy,\n",
    "                   env=env,\n",
    "                   n_steps=4096,\n",
    "                   batch_size=32,\n",
    "                   learning_rate=1e-4,\n",
    "                   tensorboard_log='logs/tensorboard',\n",
    "                   verbose=0,\n",
    "                   seed=0,\n",
    "                   **kwargs)\n",
    "\n",
    "agent = create_agent(training_env)\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we have everything we need to start the learning task! Within a couple of iterations you should see some progress in the behavior of the agent. Our model stabilizes at a total reward of around 85 which. The maximum score being 100, it means the model takes on average less than four steps to succeed (remember, we are repeated each action four actions due to the `frame_skip` parameter), that's awesome. Give yourself a tap on the back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DoomEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m evaluation_callback \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mEvalCallback(eval_env,\n\u001b[0;32m      3\u001b[0m                                              n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m                                              eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                              log_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/evaluations/basic\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                              best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/models/basic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Play!\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mppo_basic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m training_env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     12\u001b[0m eval_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:287\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    278\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    285\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 287\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:113\u001b[0m, in \u001b[0;36mVecTransposeImage.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict]:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Reset all environments\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observations, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_observations(observations)\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mojitrk\\Persistent\\Dev\\doom-drl-agent\\.conda\\Lib\\site-packages\\shimmy\\openai_gym_compatibility.py:226\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    (observation, info)\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(seed)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# Options are ignored - https://github.com/openai/gym/blob/c755d5c35a25ab118746e2ba885894ff66fb8c43/gym/core.py\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DoomEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "# Define an evaluation callback that will save the model when a new reward record is reached.\n",
    "evaluation_callback = callbacks.EvalCallback(eval_env,\n",
    "                                             n_eval_episodes=10,\n",
    "                                             eval_freq=5000,\n",
    "                                             log_path='logs/evaluations/basic',\n",
    "                                             best_model_save_path='logs/models/basic')\n",
    "\n",
    "# Play!\n",
    "agent.learn(total_timesteps=40000, tb_log_name='ppo_basic', callback=evaluation_callback)\n",
    "\n",
    "training_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained agent as a gif\n",
    "\n",
    "Now that we have an agent able to align and shoot enemies, let's make a little souvenir and save it's performance in the form of an animated gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_gif\u001b[39m(agent, file_path):\n\u001b[0;32m      4\u001b[0m     env \u001b[38;5;241m=\u001b[39m create_vec_env(frame_skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, frame_processor\u001b[38;5;241m=\u001b[39mframe_processor)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "def make_gif(agent, file_path):\n",
    "    env = create_vec_env(frame_skip=1, frame_processor=frame_processor)\n",
    "    env.venv.envs[0].game.set_seed(0)\n",
    "    \n",
    "    images = []\n",
    "\n",
    "    for i in range(20):\n",
    "        obs = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            images.append(env.venv.envs[0].game.get_state().screen_buffer)\n",
    "\n",
    "    imageio.mimsave(file_path, images, fps=35)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to pick the best model from a previous training session\n",
    "# agent = ppo.PPO.load('logs/models/basic/best_model.zip')\n",
    "make_gif(agent, 'figures/basic_agent.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the agent has the tendency to shoot too early in the gif. This is because to create a smooth-looking gif we had to create an environment where the frame skip is set to 1. We can fix this very easily by taking our trained agent and learning a few more steps but this time with the final environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_vec_env(frame_skip=1, frame_processor=frame_processor)\n",
    "\n",
    "agent.set_env(env)\n",
    "agent.learn(total_timesteps=20000)\n",
    "\n",
    "env.close()\n",
    "make_gif(agent, 'figures/basic_agent_adjusted.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison frame skip](figures/basic_agent_adjusted.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Improvements\n",
    "\n",
    "## Comparing the effects of the frame skip parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning models are notoriously hard to train. Often, a lot of care must be put into finding the correct hyperparameters for the model: learning rate, clip range, epochs per update etc. I found the frame skip setting to be one of the most influencial parameters on the learning performance. My intuitive explanation is that a larger frame skip allow the model to explore more rapidly the environment and thus encounter positive situation more often. Indeed if you try setting it to one for example, you will find that for many episodes where the monster is spawned near the border walls, the agent makes random action that tend to stay within the center of the room. This behaviour means that he is much less likely to shoot the ennemy by luck and there much less likely to receive positive rewards.\n",
    "\n",
    "I conducted a series of experiments with varying frame skip sizes, running 6 consecutive learning and averaged the results. Each error bar denotes the error mean: $ \\frac{\\sigma}{\\sqrt{n}}$.\n",
    "\n",
    "![Comparison frame skip](figures/basic_comparison_frame_skip.png)\n",
    "\n",
    "The second most influencial parameter on the learning performance is the learning rate. In most of the VizDoom examples, I found the maximum possible learning rate to be between 1e-3 and 1-4. Empirically, a learning rate that is too high seems to quickly push the agent in a state where it always outputs the same actions whereas a learning rate that is too low makes the learning procedure needlessly long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have realized that using 160 by 120 pixels input image for this scenario is completely unnecessary. The roof and the floors do not add any meaningful information to our model, only the middle of the frame posesses information. Cutting the meaningless parts means that our model will need significantly less trainable parameters. At the moment, the number of parameters is:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    print('Number of trainable parameters: {:,}'.format(\n",
    "    sum(p.numel() for p in model.policy.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 5,845,668\n"
     ]
    }
   ],
   "source": [
    "count_trainable_parameters(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we modify our environment and use instead a cropped version of the input frame, we can reduce the number of parameters by 2.6M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 3,224,228\n"
     ]
    }
   ],
   "source": [
    "frame_processor = lambda frame: cv2.resize(frame[40:-41, :], None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "env = create_vec_env(frame_processor=frame_processor); agent = create_agent(env); env.close()\n",
    "count_trainable_parameters(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the PPO class from stable-baselines allows us to directly modify the architecture of the model using keyword arguments. In particular, the default number of features produced at the end of the convolutional layers is 512. This is also unnecessarily large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 863,012\n"
     ]
    }
   ],
   "source": [
    "agent = create_agent(env, policy_kwargs={'features_extractor_kwargs': {'features_dim': 128}})\n",
    "count_trainable_parameters(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we decreased the size of trainable parameters by a factor 7. Let see how this new model compares to the old. Training this simplified model produces results on par with the more complex one while being faster to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-263.90 +/- 111.67\n",
      "Episode length: 68.30 +/- 20.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-106.10 +/- 194.05\n",
      "Episode length: 39.40 +/- 35.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=88.60 +/- 9.16\n",
      "Episode length: 3.60 +/- 2.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=76.60 +/- 23.57\n",
      "Episode length: 5.40 +/- 4.27\n",
      "Eval num_timesteps=25000, episode_reward=83.80 +/- 11.29\n",
      "Episode length: 4.80 +/- 2.82\n",
      "Eval num_timesteps=30000, episode_reward=88.60 +/- 9.83\n",
      "Episode length: 3.60 +/- 2.46\n",
      "Eval num_timesteps=35000, episode_reward=89.10 +/- 9.02\n",
      "Episode length: 3.30 +/- 2.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=83.40 +/- 10.35\n",
      "Episode length: 4.90 +/- 2.59\n"
     ]
    }
   ],
   "source": [
    "env_args = {'frame_skip': 4, 'frame_processor': frame_processor}\n",
    "\n",
    "training_env, eval_env = create_vec_env(**env_args), create_vec_env(**env_args)\n",
    "agent.set_env(training_env)\n",
    "\n",
    "evaluation_callback = callbacks.EvalCallback(eval_env,\n",
    "                                             n_eval_episodes=10,\n",
    "                                             eval_freq=5000,\n",
    "                                             log_path='logs/evaluations/basic',\n",
    "                                             best_model_save_path='logs/models/basic')\n",
    "\n",
    "agent.learn(total_timesteps=40000, tb_log_name='ppo_basic_simplified', callback=evaluation_callback)\n",
    "\n",
    "training_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we've seen how to setup and adapt VizDoom to use it as reinforcement learning environment with the powerful stable-baselines library. In the next chapter we will explore a more complex scenario and dwelve into the model in more details. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
